{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8405c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4eca697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the input data in a dataframe\n",
    "java_data = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/data_Java.csv\")\n",
    "c_data = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/data_C.csv\")\n",
    "php_data = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/data_PHP.csv\")\n",
    "python_data = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/data_Python.csv\")\n",
    "ruby_data = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/data_Ruby.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd05e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the input labels in a dataframe\n",
    "java_labels = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/label_Java.csv\")\n",
    "c_labels = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/label_C.csv\")\n",
    "php_labels = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/label_PHP.csv\")\n",
    "python_labels = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/label_Python.csv\")\n",
    "ruby_labels = pd.read_csv(\"C:/Users/victoriawu/Hackathon2021/conflict-prediction/replication_package/prediction_data/label_Ruby.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f080424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append all data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(java_data)\n",
    "df = df.append(c_data)\n",
    "df = df.append(php_data)\n",
    "df = df.append(python_data)\n",
    "df = df.append(ruby_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb539598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parallel_changed_file_num</th>\n",
       "      <th>commit_num</th>\n",
       "      <th>file_added</th>\n",
       "      <th>file_removed</th>\n",
       "      <th>file_renamed</th>\n",
       "      <th>file_modified</th>\n",
       "      <th>file_copied</th>\n",
       "      <th>line_added</th>\n",
       "      <th>line_removed</th>\n",
       "      <th>developer_num</th>\n",
       "      <th>...</th>\n",
       "      <th>fix_frequency</th>\n",
       "      <th>improve_frequency</th>\n",
       "      <th>refactor_frequency</th>\n",
       "      <th>remove_frequency</th>\n",
       "      <th>update_frequency</th>\n",
       "      <th>use_frequency</th>\n",
       "      <th>messages_min</th>\n",
       "      <th>messages_max</th>\n",
       "      <th>messages_mean</th>\n",
       "      <th>messages_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.773464e-06</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.040670</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.042394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.811897e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.114883e-05</td>\n",
       "      <td>-4.792353e-06</td>\n",
       "      <td>-0.002199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.130840</td>\n",
       "      <td>0.119701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.804461e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.687378e-07</td>\n",
       "      <td>3.018805e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104738</td>\n",
       "      <td>0.148325</td>\n",
       "      <td>0.123026</td>\n",
       "      <td>0.109726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.345800e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.453320e-05</td>\n",
       "      <td>-6.373452e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.100478</td>\n",
       "      <td>0.073566</td>\n",
       "      <td>0.073566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.383420e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.567117e-04</td>\n",
       "      <td>-7.794177e-05</td>\n",
       "      <td>-0.005865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155502</td>\n",
       "      <td>0.114476</td>\n",
       "      <td>0.112219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185214</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.014871e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.359223e-08</td>\n",
       "      <td>5.660259e-08</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>0.217703</td>\n",
       "      <td>0.118821</td>\n",
       "      <td>0.114713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.413384e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.089806e-06</td>\n",
       "      <td>-4.905558e-07</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.054863</td>\n",
       "      <td>0.145933</td>\n",
       "      <td>0.108728</td>\n",
       "      <td>0.127182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185216</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.443348e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.021344e-04</td>\n",
       "      <td>-2.449005e-05</td>\n",
       "      <td>-0.003666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082294</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.082294</td>\n",
       "      <td>0.082294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185217</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.014871e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.114563e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109726</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.109726</td>\n",
       "      <td>0.109726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185218</th>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>-0.001653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.420820e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.068155e-05</td>\n",
       "      <td>-6.792311e-06</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074813</td>\n",
       "      <td>0.150718</td>\n",
       "      <td>0.110557</td>\n",
       "      <td>0.099751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185219 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        parallel_changed_file_num  commit_num  file_added  file_removed  \\\n",
       "0                        0.000000    0.000023    0.000000      0.000021   \n",
       "1                        0.000000    0.000340   -0.000084      0.000000   \n",
       "2                        0.000000    0.000068   -0.000015      0.000007   \n",
       "3                        0.000000    0.000045   -0.000053     -0.000079   \n",
       "4                        0.000000    0.000883   -0.000496     -0.000114   \n",
       "...                           ...         ...         ...           ...   \n",
       "185214                   0.000000    0.000023    0.000000      0.000000   \n",
       "185215                   0.000000    0.000068    0.000038      0.000000   \n",
       "185216                   0.000000    0.000770   -0.000839     -0.000079   \n",
       "185217                   0.000000    0.000023    0.000000      0.000000   \n",
       "185218                   0.000178    0.000317   -0.001900     -0.001653   \n",
       "\n",
       "        file_renamed  file_modified  file_copied    line_added  line_removed  \\\n",
       "0                0.0   0.000000e+00          0.0  0.000000e+00  5.773464e-06   \n",
       "1                0.0  -4.811897e-05          0.0 -2.114883e-05 -4.792353e-06   \n",
       "2                0.0   1.804461e-06          0.0 -6.687378e-07  3.018805e-07   \n",
       "3                0.0  -2.345800e-05          0.0 -6.453320e-05 -6.373452e-05   \n",
       "4                0.0  -1.383420e-04          0.0 -2.567117e-04 -7.794177e-05   \n",
       "...              ...            ...          ...           ...           ...   \n",
       "185214           0.0   6.014871e-07          0.0  8.359223e-08  5.660259e-08   \n",
       "185215           0.0  -5.413384e-06          0.0  2.089806e-06 -4.905558e-07   \n",
       "185216           0.0  -9.443348e-05          0.0 -4.021344e-04 -2.449005e-05   \n",
       "185217           0.0   6.014871e-07          0.0  1.114563e-07  0.000000e+00   \n",
       "185218           0.0  -8.420820e-06          0.0 -4.068155e-05 -6.792311e-06   \n",
       "\n",
       "        developer_num  ...  fix_frequency  improve_frequency  \\\n",
       "0            0.000733  ...       0.000000           0.000000   \n",
       "1           -0.002199  ...       0.083333           0.000000   \n",
       "2            0.000000  ...       0.000000           0.000000   \n",
       "3            0.000000  ...       0.000000           0.000000   \n",
       "4           -0.005865  ...       0.083333           0.153846   \n",
       "...               ...  ...            ...                ...   \n",
       "185214       0.000733  ...       0.166667           0.000000   \n",
       "185215      -0.000733  ...       0.000000           0.076923   \n",
       "185216      -0.003666  ...       0.000000           0.000000   \n",
       "185217       0.000733  ...       0.000000           0.000000   \n",
       "185218      -0.000733  ...       0.041667           0.000000   \n",
       "\n",
       "        refactor_frequency  remove_frequency  update_frequency  use_frequency  \\\n",
       "0                      0.0          0.041667          0.000000       0.000000   \n",
       "1                      0.0          0.041667          0.000000       0.086957   \n",
       "2                      0.0          0.000000          0.022222       0.000000   \n",
       "3                      0.0          0.000000          0.022222       0.000000   \n",
       "4                      0.0          0.083333          0.000000       0.043478   \n",
       "...                    ...               ...               ...            ...   \n",
       "185214                 0.0          0.041667          0.000000       0.000000   \n",
       "185215                 0.0          0.083333          0.000000       0.086957   \n",
       "185216                 0.0          0.000000          0.000000       0.000000   \n",
       "185217                 0.0          0.000000          0.000000       0.000000   \n",
       "185218                 0.0          0.000000          0.000000       0.000000   \n",
       "\n",
       "        messages_min  messages_max  messages_mean  messages_median  \n",
       "0           0.042394      0.040670       0.042394         0.042394  \n",
       "1           0.039900      0.318182       0.130840         0.119701  \n",
       "2           0.104738      0.148325       0.123026         0.109726  \n",
       "3           0.042394      0.100478       0.073566         0.073566  \n",
       "4           0.000000      0.155502       0.114476         0.112219  \n",
       "...              ...           ...            ...              ...  \n",
       "185214      0.064838      0.217703       0.118821         0.114713  \n",
       "185215      0.054863      0.145933       0.108728         0.127182  \n",
       "185216      0.082294      0.078947       0.082294         0.082294  \n",
       "185217      0.109726      0.105263       0.109726         0.109726  \n",
       "185218      0.074813      0.150718       0.110557         0.099751  \n",
       "\n",
       "[185219 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an abs_scaler object\n",
    "abs_scaler = MaxAbsScaler()\n",
    "\n",
    "# calculate the maximum absolute value for scaling the data using the fit method\n",
    "abs_scaler.fit(df)\n",
    "\n",
    "# transform the data using the parameters calculated by the fit method (the maximum absolute values)\n",
    "scaled_data = abs_scaler.transform(df)\n",
    "\n",
    "# store the results in a data frame\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# visualize the data frame\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "687a455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all labels into one dataframe\n",
    "labelsdf = pd.DataFrame()\n",
    "labelsdf = labelsdf.append(java_labels)\n",
    "labelsdf = labelsdf.append(c_labels)\n",
    "labelsdf = labelsdf.append(php_labels)\n",
    "labelsdf = labelsdf.append(python_labels)\n",
    "labelsdf = labelsdf.append(ruby_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8a24141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parallel_changed_file_num</th>\n",
       "      <th>commit_num</th>\n",
       "      <th>file_added</th>\n",
       "      <th>file_removed</th>\n",
       "      <th>file_renamed</th>\n",
       "      <th>file_modified</th>\n",
       "      <th>file_copied</th>\n",
       "      <th>line_added</th>\n",
       "      <th>line_removed</th>\n",
       "      <th>developer_num</th>\n",
       "      <th>...</th>\n",
       "      <th>improve_frequency</th>\n",
       "      <th>refactor_frequency</th>\n",
       "      <th>remove_frequency</th>\n",
       "      <th>update_frequency</th>\n",
       "      <th>use_frequency</th>\n",
       "      <th>messages_min</th>\n",
       "      <th>messages_max</th>\n",
       "      <th>messages_mean</th>\n",
       "      <th>messages_median</th>\n",
       "      <th>is_conflict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.773464e-06</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.040670</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.811897e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.114883e-05</td>\n",
       "      <td>-4.792353e-06</td>\n",
       "      <td>-0.002199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.130840</td>\n",
       "      <td>0.119701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.804461e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.687378e-07</td>\n",
       "      <td>3.018805e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104738</td>\n",
       "      <td>0.148325</td>\n",
       "      <td>0.123026</td>\n",
       "      <td>0.109726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.345800e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.453320e-05</td>\n",
       "      <td>-6.373452e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.100478</td>\n",
       "      <td>0.073566</td>\n",
       "      <td>0.073566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.383420e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.567117e-04</td>\n",
       "      <td>-7.794177e-05</td>\n",
       "      <td>-0.005865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155502</td>\n",
       "      <td>0.114476</td>\n",
       "      <td>0.112219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185214</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.014871e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.359223e-08</td>\n",
       "      <td>5.660259e-08</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>0.217703</td>\n",
       "      <td>0.118821</td>\n",
       "      <td>0.114713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.413384e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.089806e-06</td>\n",
       "      <td>-4.905558e-07</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.054863</td>\n",
       "      <td>0.145933</td>\n",
       "      <td>0.108728</td>\n",
       "      <td>0.127182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185216</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.443348e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.021344e-04</td>\n",
       "      <td>-2.449005e-05</td>\n",
       "      <td>-0.003666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082294</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.082294</td>\n",
       "      <td>0.082294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185217</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.014871e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.114563e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109726</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.109726</td>\n",
       "      <td>0.109726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185218</th>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>-0.001653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.420820e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.068155e-05</td>\n",
       "      <td>-6.792311e-06</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074813</td>\n",
       "      <td>0.150718</td>\n",
       "      <td>0.110557</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185219 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        parallel_changed_file_num  commit_num  file_added  file_removed  \\\n",
       "0                        0.000000    0.000023    0.000000      0.000021   \n",
       "1                        0.000000    0.000340   -0.000084      0.000000   \n",
       "2                        0.000000    0.000068   -0.000015      0.000007   \n",
       "3                        0.000000    0.000045   -0.000053     -0.000079   \n",
       "4                        0.000000    0.000883   -0.000496     -0.000114   \n",
       "...                           ...         ...         ...           ...   \n",
       "185214                   0.000000    0.000023    0.000000      0.000000   \n",
       "185215                   0.000000    0.000068    0.000038      0.000000   \n",
       "185216                   0.000000    0.000770   -0.000839     -0.000079   \n",
       "185217                   0.000000    0.000023    0.000000      0.000000   \n",
       "185218                   0.000178    0.000317   -0.001900     -0.001653   \n",
       "\n",
       "        file_renamed  file_modified  file_copied    line_added  line_removed  \\\n",
       "0                0.0   0.000000e+00          0.0  0.000000e+00  5.773464e-06   \n",
       "1                0.0  -4.811897e-05          0.0 -2.114883e-05 -4.792353e-06   \n",
       "2                0.0   1.804461e-06          0.0 -6.687378e-07  3.018805e-07   \n",
       "3                0.0  -2.345800e-05          0.0 -6.453320e-05 -6.373452e-05   \n",
       "4                0.0  -1.383420e-04          0.0 -2.567117e-04 -7.794177e-05   \n",
       "...              ...            ...          ...           ...           ...   \n",
       "185214           0.0   6.014871e-07          0.0  8.359223e-08  5.660259e-08   \n",
       "185215           0.0  -5.413384e-06          0.0  2.089806e-06 -4.905558e-07   \n",
       "185216           0.0  -9.443348e-05          0.0 -4.021344e-04 -2.449005e-05   \n",
       "185217           0.0   6.014871e-07          0.0  1.114563e-07  0.000000e+00   \n",
       "185218           0.0  -8.420820e-06          0.0 -4.068155e-05 -6.792311e-06   \n",
       "\n",
       "        developer_num  ...  improve_frequency  refactor_frequency  \\\n",
       "0            0.000733  ...           0.000000                 0.0   \n",
       "1           -0.002199  ...           0.000000                 0.0   \n",
       "2            0.000000  ...           0.000000                 0.0   \n",
       "3            0.000000  ...           0.000000                 0.0   \n",
       "4           -0.005865  ...           0.153846                 0.0   \n",
       "...               ...  ...                ...                 ...   \n",
       "185214       0.000733  ...           0.000000                 0.0   \n",
       "185215      -0.000733  ...           0.076923                 0.0   \n",
       "185216      -0.003666  ...           0.000000                 0.0   \n",
       "185217       0.000733  ...           0.000000                 0.0   \n",
       "185218      -0.000733  ...           0.000000                 0.0   \n",
       "\n",
       "        remove_frequency  update_frequency  use_frequency  messages_min  \\\n",
       "0               0.041667          0.000000       0.000000      0.042394   \n",
       "1               0.041667          0.000000       0.086957      0.039900   \n",
       "2               0.000000          0.022222       0.000000      0.104738   \n",
       "3               0.000000          0.022222       0.000000      0.042394   \n",
       "4               0.083333          0.000000       0.043478      0.000000   \n",
       "...                  ...               ...            ...           ...   \n",
       "185214          0.041667          0.000000       0.000000      0.064838   \n",
       "185215          0.083333          0.000000       0.086957      0.054863   \n",
       "185216          0.000000          0.000000       0.000000      0.082294   \n",
       "185217          0.000000          0.000000       0.000000      0.109726   \n",
       "185218          0.000000          0.000000       0.000000      0.074813   \n",
       "\n",
       "        messages_max  messages_mean  messages_median  is_conflict  \n",
       "0           0.040670       0.042394         0.042394            0  \n",
       "1           0.318182       0.130840         0.119701            0  \n",
       "2           0.148325       0.123026         0.109726            0  \n",
       "3           0.100478       0.073566         0.073566            0  \n",
       "4           0.155502       0.114476         0.112219            0  \n",
       "...              ...            ...              ...          ...  \n",
       "185214      0.217703       0.118821         0.114713            0  \n",
       "185215      0.145933       0.108728         0.127182            0  \n",
       "185216      0.078947       0.082294         0.082294            0  \n",
       "185217      0.105263       0.109726         0.109726            0  \n",
       "185218      0.150718       0.110557         0.099751            0  \n",
       "\n",
       "[185219 rows x 29 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Append labels to the training data \n",
    "df_scaled.insert(28, 'is_conflict', labelsdf)\n",
    "\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86fa8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the data to randomize the languages\n",
    "df_scaled = df_scaled.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a27a11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (80%) and testing (20%)\n",
    "train, test = train_test_split(df_scaled, test_size=0.2)\n",
    "\n",
    "# Split data into features and labels (only using the first 29 columns as 30 is the amount and 31 is the label\n",
    "train_features = np.array(train.values[:,:27])\n",
    "train_labels = np.array(train.values[:,-1])\n",
    "test_features = np.array(test.values[:,:27])\n",
    "test_labels = np.array(test.values[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43ad695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=100, kernel_initializer='uniform', input_dim=train_features.shape[1], activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(units=100, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea06e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2811 - accuracy: 0.9198 - val_loss: 0.2569 - val_accuracy: 0.9239\n",
      "Epoch 2/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2575 - accuracy: 0.9197 - val_loss: 0.2365 - val_accuracy: 0.9241\n",
      "Epoch 3/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2366 - accuracy: 0.9193 - val_loss: 0.2197 - val_accuracy: 0.9246\n",
      "Epoch 4/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2259 - accuracy: 0.9202 - val_loss: 0.2068 - val_accuracy: 0.9258\n",
      "Epoch 5/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2193 - accuracy: 0.9207 - val_loss: 0.2039 - val_accuracy: 0.9245\n",
      "Epoch 6/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2144 - accuracy: 0.9211 - val_loss: 0.2007 - val_accuracy: 0.9261\n",
      "Epoch 7/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2100 - accuracy: 0.9217 - val_loss: 0.2039 - val_accuracy: 0.9261\n",
      "Epoch 8/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2058 - accuracy: 0.9223 - val_loss: 0.2142 - val_accuracy: 0.9276\n",
      "Epoch 9/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.2017 - accuracy: 0.9234 - val_loss: 0.2083 - val_accuracy: 0.9271\n",
      "Epoch 10/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1985 - accuracy: 0.9247 - val_loss: 0.2006 - val_accuracy: 0.9300\n",
      "Epoch 11/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1948 - accuracy: 0.9267 - val_loss: 0.1982 - val_accuracy: 0.9299\n",
      "Epoch 12/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1910 - accuracy: 0.9286 - val_loss: 0.1840 - val_accuracy: 0.9328\n",
      "Epoch 13/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1877 - accuracy: 0.9304 - val_loss: 0.1847 - val_accuracy: 0.9325\n",
      "Epoch 14/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.1855 - accuracy: 0.9303 - val_loss: 0.1780 - val_accuracy: 0.9341\n",
      "Epoch 15/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1829 - accuracy: 0.9309 - val_loss: 0.1720 - val_accuracy: 0.9348\n",
      "Epoch 16/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1809 - accuracy: 0.9320 - val_loss: 0.1902 - val_accuracy: 0.9328\n",
      "Epoch 17/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.1792 - accuracy: 0.9320 - val_loss: 0.1625 - val_accuracy: 0.9355\n",
      "Epoch 18/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.1792 - accuracy: 0.9323 - val_loss: 0.1647 - val_accuracy: 0.9351\n",
      "Epoch 19/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.1780 - accuracy: 0.9328 - val_loss: 0.1825 - val_accuracy: 0.9312\n",
      "Epoch 20/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1764 - accuracy: 0.9326 - val_loss: 0.1609 - val_accuracy: 0.9361\n",
      "Epoch 21/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1774 - accuracy: 0.9325 - val_loss: 0.1651 - val_accuracy: 0.9360\n",
      "Epoch 22/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1750 - accuracy: 0.9328 - val_loss: 0.1608 - val_accuracy: 0.9355\n",
      "Epoch 23/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1750 - accuracy: 0.9324 - val_loss: 0.1595 - val_accuracy: 0.9356\n",
      "Epoch 24/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1743 - accuracy: 0.9328 - val_loss: 0.1603 - val_accuracy: 0.9354\n",
      "Epoch 25/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1740 - accuracy: 0.9331 - val_loss: 0.1577 - val_accuracy: 0.9359\n",
      "Epoch 26/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1735 - accuracy: 0.9326 - val_loss: 0.1580 - val_accuracy: 0.9362\n",
      "Epoch 27/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1743 - accuracy: 0.9326 - val_loss: 0.1697 - val_accuracy: 0.9328\n",
      "Epoch 28/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1735 - accuracy: 0.9333 - val_loss: 0.1604 - val_accuracy: 0.9355\n",
      "Epoch 29/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1739 - accuracy: 0.9323 - val_loss: 0.1624 - val_accuracy: 0.9346\n",
      "Epoch 30/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1726 - accuracy: 0.9328 - val_loss: 0.1618 - val_accuracy: 0.9366\n",
      "Epoch 31/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1718 - accuracy: 0.9324 - val_loss: 0.1593 - val_accuracy: 0.9345\n",
      "Epoch 32/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1726 - accuracy: 0.9328 - val_loss: 0.1702 - val_accuracy: 0.9358\n",
      "Epoch 33/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1710 - accuracy: 0.9327 - val_loss: 0.1559 - val_accuracy: 0.9367\n",
      "Epoch 34/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1706 - accuracy: 0.9322 - val_loss: 0.1593 - val_accuracy: 0.9354\n",
      "Epoch 35/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1711 - accuracy: 0.9326 - val_loss: 0.1562 - val_accuracy: 0.9362\n",
      "Epoch 36/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1703 - accuracy: 0.9323 - val_loss: 0.1596 - val_accuracy: 0.9367\n",
      "Epoch 37/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1713 - accuracy: 0.9328 - val_loss: 0.1606 - val_accuracy: 0.9364\n",
      "Epoch 38/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1712 - accuracy: 0.9321 - val_loss: 0.1569 - val_accuracy: 0.9358\n",
      "Epoch 39/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1704 - accuracy: 0.9324 - val_loss: 0.1561 - val_accuracy: 0.9360\n",
      "Epoch 40/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1700 - accuracy: 0.9323 - val_loss: 0.1566 - val_accuracy: 0.9365\n",
      "Epoch 41/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1696 - accuracy: 0.9327 - val_loss: 0.1554 - val_accuracy: 0.9358\n",
      "Epoch 42/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1684 - accuracy: 0.9326 - val_loss: 0.1565 - val_accuracy: 0.9367\n",
      "Epoch 43/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1689 - accuracy: 0.9332 - val_loss: 0.1567 - val_accuracy: 0.9369\n",
      "Epoch 44/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1706 - accuracy: 0.9330 - val_loss: 0.1560 - val_accuracy: 0.9368\n",
      "Epoch 45/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1697 - accuracy: 0.9322 - val_loss: 0.1658 - val_accuracy: 0.9363\n",
      "Epoch 46/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1690 - accuracy: 0.9330 - val_loss: 0.1542 - val_accuracy: 0.9365\n",
      "Epoch 47/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1696 - accuracy: 0.9327 - val_loss: 0.1600 - val_accuracy: 0.9366\n",
      "Epoch 48/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1685 - accuracy: 0.9331 - val_loss: 0.1566 - val_accuracy: 0.9355\n",
      "Epoch 49/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1681 - accuracy: 0.9330 - val_loss: 0.1593 - val_accuracy: 0.9365\n",
      "Epoch 50/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1684 - accuracy: 0.9326 - val_loss: 0.1551 - val_accuracy: 0.9365\n",
      "Epoch 51/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1682 - accuracy: 0.9328 - val_loss: 0.1590 - val_accuracy: 0.9365\n",
      "Epoch 52/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1669 - accuracy: 0.9329 - val_loss: 0.1559 - val_accuracy: 0.9358\n",
      "Epoch 53/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1680 - accuracy: 0.9330 - val_loss: 0.1659 - val_accuracy: 0.9366\n",
      "Epoch 54/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1683 - accuracy: 0.9327 - val_loss: 0.1627 - val_accuracy: 0.9357\n",
      "Epoch 55/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1697 - accuracy: 0.9326 - val_loss: 0.1642 - val_accuracy: 0.9358\n",
      "Epoch 56/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1690 - accuracy: 0.9330 - val_loss: 0.1710 - val_accuracy: 0.9354\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1682 - accuracy: 0.9325 - val_loss: 0.1566 - val_accuracy: 0.9355\n",
      "Epoch 58/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1685 - accuracy: 0.9330 - val_loss: 0.1557 - val_accuracy: 0.9367\n",
      "Epoch 59/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1680 - accuracy: 0.9329 - val_loss: 0.1555 - val_accuracy: 0.9366\n",
      "Epoch 60/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1671 - accuracy: 0.9327 - val_loss: 0.1653 - val_accuracy: 0.9355\n",
      "Epoch 61/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1673 - accuracy: 0.9330 - val_loss: 0.1562 - val_accuracy: 0.9330\n",
      "Epoch 62/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1672 - accuracy: 0.9327 - val_loss: 0.1566 - val_accuracy: 0.9350\n",
      "Epoch 63/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1673 - accuracy: 0.9330 - val_loss: 0.1584 - val_accuracy: 0.9362\n",
      "Epoch 64/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1664 - accuracy: 0.9335 - val_loss: 0.1589 - val_accuracy: 0.9363\n",
      "Epoch 65/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1660 - accuracy: 0.9331 - val_loss: 0.1545 - val_accuracy: 0.9353\n",
      "Epoch 66/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1663 - accuracy: 0.9335 - val_loss: 0.1591 - val_accuracy: 0.9364\n",
      "Epoch 67/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1661 - accuracy: 0.9327 - val_loss: 0.1555 - val_accuracy: 0.9359\n",
      "Epoch 68/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1666 - accuracy: 0.9330 - val_loss: 0.1611 - val_accuracy: 0.9361\n",
      "Epoch 69/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1672 - accuracy: 0.9329 - val_loss: 0.1557 - val_accuracy: 0.9349\n",
      "Epoch 70/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1660 - accuracy: 0.9327 - val_loss: 0.1544 - val_accuracy: 0.9365\n",
      "Epoch 71/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1670 - accuracy: 0.9327 - val_loss: 0.1558 - val_accuracy: 0.9362\n",
      "Epoch 72/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1660 - accuracy: 0.9329 - val_loss: 0.1583 - val_accuracy: 0.9346\n",
      "Epoch 73/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1667 - accuracy: 0.9325 - val_loss: 0.1563 - val_accuracy: 0.9367\n",
      "Epoch 74/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1659 - accuracy: 0.9329 - val_loss: 0.1613 - val_accuracy: 0.9352\n",
      "Epoch 75/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1661 - accuracy: 0.9327 - val_loss: 0.1582 - val_accuracy: 0.9363\n",
      "Epoch 76/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1661 - accuracy: 0.9329 - val_loss: 0.1532 - val_accuracy: 0.9367\n",
      "Epoch 77/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1654 - accuracy: 0.9335 - val_loss: 0.1551 - val_accuracy: 0.9351\n",
      "Epoch 78/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1670 - accuracy: 0.9327 - val_loss: 0.1571 - val_accuracy: 0.9362\n",
      "Epoch 79/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1668 - accuracy: 0.9328 - val_loss: 0.1578 - val_accuracy: 0.9356\n",
      "Epoch 80/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1679 - accuracy: 0.9330 - val_loss: 0.1552 - val_accuracy: 0.9357\n",
      "Epoch 81/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1658 - accuracy: 0.9336 - val_loss: 0.1538 - val_accuracy: 0.9363\n",
      "Epoch 82/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1657 - accuracy: 0.9330 - val_loss: 0.1569 - val_accuracy: 0.9353\n",
      "Epoch 83/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1667 - accuracy: 0.9328 - val_loss: 0.1588 - val_accuracy: 0.9349\n",
      "Epoch 84/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1662 - accuracy: 0.9329 - val_loss: 0.1568 - val_accuracy: 0.9353\n",
      "Epoch 85/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1673 - accuracy: 0.9328 - val_loss: 0.1574 - val_accuracy: 0.9355\n",
      "Epoch 86/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1664 - accuracy: 0.9328 - val_loss: 0.1570 - val_accuracy: 0.9352\n",
      "Epoch 87/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1663 - accuracy: 0.9334 - val_loss: 0.1553 - val_accuracy: 0.9360\n",
      "Epoch 88/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1650 - accuracy: 0.9328 - val_loss: 0.1538 - val_accuracy: 0.9360\n",
      "Epoch 89/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1667 - accuracy: 0.9334 - val_loss: 0.1550 - val_accuracy: 0.9352\n",
      "Epoch 90/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1661 - accuracy: 0.9321 - val_loss: 0.1539 - val_accuracy: 0.9360\n",
      "Epoch 91/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1653 - accuracy: 0.9332 - val_loss: 0.1593 - val_accuracy: 0.9361\n",
      "Epoch 92/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1640 - accuracy: 0.9334 - val_loss: 0.1548 - val_accuracy: 0.9361\n",
      "Epoch 93/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1648 - accuracy: 0.9338 - val_loss: 0.1541 - val_accuracy: 0.9365\n",
      "Epoch 94/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1652 - accuracy: 0.9327 - val_loss: 0.1554 - val_accuracy: 0.9367\n",
      "Epoch 95/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1648 - accuracy: 0.9331 - val_loss: 0.1740 - val_accuracy: 0.9359\n",
      "Epoch 96/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1657 - accuracy: 0.9328 - val_loss: 0.1581 - val_accuracy: 0.9352\n",
      "Epoch 97/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1650 - accuracy: 0.9327 - val_loss: 0.1544 - val_accuracy: 0.9370\n",
      "Epoch 98/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1644 - accuracy: 0.9335 - val_loss: 0.1553 - val_accuracy: 0.9363\n",
      "Epoch 99/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1644 - accuracy: 0.9328 - val_loss: 0.1700 - val_accuracy: 0.9316\n",
      "Epoch 100/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1653 - accuracy: 0.9329 - val_loss: 0.1558 - val_accuracy: 0.9353\n",
      "Epoch 101/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1646 - accuracy: 0.9328 - val_loss: 0.1556 - val_accuracy: 0.9363\n",
      "Epoch 102/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1633 - accuracy: 0.9333 - val_loss: 0.1545 - val_accuracy: 0.9355\n",
      "Epoch 103/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1650 - accuracy: 0.9333 - val_loss: 0.1593 - val_accuracy: 0.9350\n",
      "Epoch 104/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1648 - accuracy: 0.9334 - val_loss: 0.1557 - val_accuracy: 0.9362\n",
      "Epoch 105/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1650 - accuracy: 0.9333 - val_loss: 0.1821 - val_accuracy: 0.9353\n",
      "Epoch 106/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1637 - accuracy: 0.9334 - val_loss: 0.1616 - val_accuracy: 0.9358\n",
      "Epoch 107/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1650 - accuracy: 0.9333 - val_loss: 0.1570 - val_accuracy: 0.9370\n",
      "Epoch 108/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1646 - accuracy: 0.9328 - val_loss: 0.1692 - val_accuracy: 0.9364\n",
      "Epoch 109/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1643 - accuracy: 0.9336 - val_loss: 0.1561 - val_accuracy: 0.9362\n",
      "Epoch 110/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1639 - accuracy: 0.9329 - val_loss: 0.1569 - val_accuracy: 0.9354\n",
      "Epoch 111/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1633 - accuracy: 0.9335 - val_loss: 0.1699 - val_accuracy: 0.9344\n",
      "Epoch 112/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1629 - accuracy: 0.9333 - val_loss: 0.1558 - val_accuracy: 0.9360\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1631 - accuracy: 0.9337 - val_loss: 0.1596 - val_accuracy: 0.9355\n",
      "Epoch 114/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1631 - accuracy: 0.9338 - val_loss: 0.1542 - val_accuracy: 0.9358\n",
      "Epoch 115/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1627 - accuracy: 0.9335 - val_loss: 0.1579 - val_accuracy: 0.9352\n",
      "Epoch 116/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1629 - accuracy: 0.9327 - val_loss: 0.1723 - val_accuracy: 0.9355\n",
      "Epoch 117/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1650 - accuracy: 0.9329 - val_loss: 0.1562 - val_accuracy: 0.9345\n",
      "Epoch 118/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1633 - accuracy: 0.9341 - val_loss: 0.1547 - val_accuracy: 0.9358\n",
      "Epoch 119/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1620 - accuracy: 0.9341 - val_loss: 0.1555 - val_accuracy: 0.9359\n",
      "Epoch 120/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1636 - accuracy: 0.9337 - val_loss: 0.1631 - val_accuracy: 0.9358\n",
      "Epoch 121/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1641 - accuracy: 0.9336 - val_loss: 0.1591 - val_accuracy: 0.9348\n",
      "Epoch 122/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1637 - accuracy: 0.9330 - val_loss: 0.1583 - val_accuracy: 0.9357\n",
      "Epoch 123/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1624 - accuracy: 0.9335 - val_loss: 0.1553 - val_accuracy: 0.9361\n",
      "Epoch 124/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1637 - accuracy: 0.9334 - val_loss: 0.1578 - val_accuracy: 0.9358\n",
      "Epoch 125/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1627 - accuracy: 0.9340 - val_loss: 0.1567 - val_accuracy: 0.9360\n",
      "Epoch 126/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1627 - accuracy: 0.9336 - val_loss: 0.1578 - val_accuracy: 0.9350\n",
      "Epoch 127/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1630 - accuracy: 0.9339 - val_loss: 0.1563 - val_accuracy: 0.9361\n",
      "Epoch 128/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1624 - accuracy: 0.9338 - val_loss: 0.1567 - val_accuracy: 0.9363\n",
      "Epoch 129/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1622 - accuracy: 0.9339 - val_loss: 0.1580 - val_accuracy: 0.9352\n",
      "Epoch 130/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1617 - accuracy: 0.9343 - val_loss: 0.1553 - val_accuracy: 0.9361\n",
      "Epoch 131/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1627 - accuracy: 0.9337 - val_loss: 0.1554 - val_accuracy: 0.9366\n",
      "Epoch 132/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1623 - accuracy: 0.9338 - val_loss: 0.1590 - val_accuracy: 0.9363\n",
      "Epoch 133/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1612 - accuracy: 0.9342 - val_loss: 0.1595 - val_accuracy: 0.9364\n",
      "Epoch 134/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1621 - accuracy: 0.9339 - val_loss: 0.1572 - val_accuracy: 0.9357\n",
      "Epoch 135/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1616 - accuracy: 0.9345 - val_loss: 0.1573 - val_accuracy: 0.9362\n",
      "Epoch 136/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1617 - accuracy: 0.9344 - val_loss: 0.1554 - val_accuracy: 0.9364\n",
      "Epoch 137/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1623 - accuracy: 0.9344 - val_loss: 0.1575 - val_accuracy: 0.9363\n",
      "Epoch 138/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1609 - accuracy: 0.9342 - val_loss: 0.1632 - val_accuracy: 0.9325\n",
      "Epoch 139/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1632 - accuracy: 0.9336 - val_loss: 0.1573 - val_accuracy: 0.9345\n",
      "Epoch 140/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1621 - accuracy: 0.9340 - val_loss: 0.1557 - val_accuracy: 0.9362\n",
      "Epoch 141/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1607 - accuracy: 0.9339 - val_loss: 0.1550 - val_accuracy: 0.9359\n",
      "Epoch 142/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1613 - accuracy: 0.9339 - val_loss: 0.1626 - val_accuracy: 0.9364\n",
      "Epoch 143/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1612 - accuracy: 0.9346 - val_loss: 0.1566 - val_accuracy: 0.9365\n",
      "Epoch 144/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1609 - accuracy: 0.9343 - val_loss: 0.1601 - val_accuracy: 0.9363\n",
      "Epoch 145/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1609 - accuracy: 0.9342 - val_loss: 0.1591 - val_accuracy: 0.9363\n",
      "Epoch 146/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1623 - accuracy: 0.9338 - val_loss: 0.1583 - val_accuracy: 0.9362\n",
      "Epoch 147/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1620 - accuracy: 0.9338 - val_loss: 0.1646 - val_accuracy: 0.9364\n",
      "Epoch 148/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1614 - accuracy: 0.9340 - val_loss: 0.1552 - val_accuracy: 0.9359\n",
      "Epoch 149/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1617 - accuracy: 0.9339 - val_loss: 0.1554 - val_accuracy: 0.9356\n",
      "Epoch 150/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1615 - accuracy: 0.9341 - val_loss: 0.1578 - val_accuracy: 0.9357\n",
      "Epoch 151/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1612 - accuracy: 0.9336 - val_loss: 0.1591 - val_accuracy: 0.9354\n",
      "Epoch 152/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1614 - accuracy: 0.9342 - val_loss: 0.1557 - val_accuracy: 0.9358\n",
      "Epoch 153/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1611 - accuracy: 0.9339 - val_loss: 0.1565 - val_accuracy: 0.9360\n",
      "Epoch 154/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1613 - accuracy: 0.9344 - val_loss: 0.1614 - val_accuracy: 0.9355\n",
      "Epoch 155/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1608 - accuracy: 0.9341 - val_loss: 0.1585 - val_accuracy: 0.9345\n",
      "Epoch 156/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1600 - accuracy: 0.9340 - val_loss: 0.1541 - val_accuracy: 0.9366\n",
      "Epoch 157/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1610 - accuracy: 0.9338 - val_loss: 0.1553 - val_accuracy: 0.9361\n",
      "Epoch 158/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1605 - accuracy: 0.9341 - val_loss: 0.1585 - val_accuracy: 0.9353\n",
      "Epoch 159/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1616 - accuracy: 0.9348 - val_loss: 0.1581 - val_accuracy: 0.9365\n",
      "Epoch 160/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1601 - accuracy: 0.9345 - val_loss: 0.1567 - val_accuracy: 0.9346\n",
      "Epoch 161/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1608 - accuracy: 0.9341 - val_loss: 0.1542 - val_accuracy: 0.9367\n",
      "Epoch 162/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1611 - accuracy: 0.9341 - val_loss: 0.1600 - val_accuracy: 0.9370\n",
      "Epoch 163/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1607 - accuracy: 0.9335 - val_loss: 0.1627 - val_accuracy: 0.9357\n",
      "Epoch 164/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1601 - accuracy: 0.9339 - val_loss: 0.1565 - val_accuracy: 0.9365\n",
      "Epoch 165/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1609 - accuracy: 0.9340 - val_loss: 0.1559 - val_accuracy: 0.9353\n",
      "Epoch 166/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1601 - accuracy: 0.9344 - val_loss: 0.1570 - val_accuracy: 0.9358\n",
      "Epoch 167/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1594 - accuracy: 0.9345 - val_loss: 0.1559 - val_accuracy: 0.9363\n",
      "Epoch 168/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1605 - accuracy: 0.9340 - val_loss: 0.1567 - val_accuracy: 0.9363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1604 - accuracy: 0.9345 - val_loss: 0.1545 - val_accuracy: 0.9365\n",
      "Epoch 170/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1603 - accuracy: 0.9336 - val_loss: 0.1602 - val_accuracy: 0.9349\n",
      "Epoch 171/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1603 - accuracy: 0.9341 - val_loss: 0.1554 - val_accuracy: 0.9365\n",
      "Epoch 172/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1604 - accuracy: 0.9342 - val_loss: 0.1552 - val_accuracy: 0.9360\n",
      "Epoch 173/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1600 - accuracy: 0.9342 - val_loss: 0.1572 - val_accuracy: 0.9367\n",
      "Epoch 174/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1604 - accuracy: 0.9343 - val_loss: 0.1604 - val_accuracy: 0.9362\n",
      "Epoch 175/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1595 - accuracy: 0.9345 - val_loss: 0.1566 - val_accuracy: 0.9368\n",
      "Epoch 176/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1598 - accuracy: 0.9343 - val_loss: 0.1563 - val_accuracy: 0.9369\n",
      "Epoch 177/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1596 - accuracy: 0.9341 - val_loss: 0.1556 - val_accuracy: 0.9351\n",
      "Epoch 178/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1601 - accuracy: 0.9344 - val_loss: 0.1554 - val_accuracy: 0.9363\n",
      "Epoch 179/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1598 - accuracy: 0.9341 - val_loss: 0.1596 - val_accuracy: 0.9357\n",
      "Epoch 180/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1593 - accuracy: 0.9345 - val_loss: 0.1563 - val_accuracy: 0.9349\n",
      "Epoch 181/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1598 - accuracy: 0.9341 - val_loss: 0.1558 - val_accuracy: 0.9358\n",
      "Epoch 182/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1606 - accuracy: 0.9344 - val_loss: 0.1556 - val_accuracy: 0.9356\n",
      "Epoch 183/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1599 - accuracy: 0.9344 - val_loss: 0.1563 - val_accuracy: 0.9363\n",
      "Epoch 184/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1600 - accuracy: 0.9346 - val_loss: 0.1568 - val_accuracy: 0.9358\n",
      "Epoch 185/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1600 - accuracy: 0.9343 - val_loss: 0.1542 - val_accuracy: 0.9357\n",
      "Epoch 186/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1602 - accuracy: 0.9340 - val_loss: 0.1560 - val_accuracy: 0.9359\n",
      "Epoch 187/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1597 - accuracy: 0.9344 - val_loss: 0.1551 - val_accuracy: 0.9350\n",
      "Epoch 188/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1598 - accuracy: 0.9345 - val_loss: 0.1559 - val_accuracy: 0.9363\n",
      "Epoch 189/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1607 - accuracy: 0.9350 - val_loss: 0.1564 - val_accuracy: 0.9360\n",
      "Epoch 190/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1596 - accuracy: 0.9346 - val_loss: 0.1627 - val_accuracy: 0.9350\n",
      "Epoch 191/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1596 - accuracy: 0.9347 - val_loss: 0.1561 - val_accuracy: 0.9361\n",
      "Epoch 192/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1592 - accuracy: 0.9351 - val_loss: 0.1579 - val_accuracy: 0.9348\n",
      "Epoch 193/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1591 - accuracy: 0.9350 - val_loss: 0.1578 - val_accuracy: 0.9356\n",
      "Epoch 194/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1593 - accuracy: 0.9343 - val_loss: 0.1550 - val_accuracy: 0.9364\n",
      "Epoch 195/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1596 - accuracy: 0.9340 - val_loss: 0.1568 - val_accuracy: 0.9359\n",
      "Epoch 196/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1597 - accuracy: 0.9343 - val_loss: 0.1585 - val_accuracy: 0.9359\n",
      "Epoch 197/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1596 - accuracy: 0.9340 - val_loss: 0.1561 - val_accuracy: 0.9365\n",
      "Epoch 198/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1593 - accuracy: 0.9348 - val_loss: 0.1572 - val_accuracy: 0.9363\n",
      "Epoch 199/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1598 - accuracy: 0.9346 - val_loss: 0.1552 - val_accuracy: 0.9355\n",
      "Epoch 200/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.1589 - accuracy: 0.9348 - val_loss: 0.1601 - val_accuracy: 0.9356\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(train_features, train_labels, batch_size=50, epochs=200, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b24e00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158/1158 [==============================] - 1s 986us/step - loss: 0.1628 - accuracy: 0.9349\n",
      "\n",
      "\n",
      "accuracy= 0.9348612427711487\n",
      "WARNING:tensorflow:From C:\\Users\\VICTOR~1\\AppData\\Local\\Temp/ipykernel_1468/1264745586.py:7: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "Predicted      0     1\n",
      "Actual                \n",
      "0.0        33423   646\n",
      "1.0         1767  1208\n",
      "Num Fraud: 2975\n",
      "Precision:  0.651564185544768\n",
      "Recall/Sensitivity:  0.40605042016806725\n",
      "Specificity:  0.9810384807302827\n",
      "F1 Score:  0.500310623317457\n"
     ]
    }
   ],
   "source": [
    "# Get model accuracy \n",
    "scores = model.evaluate(test_features, test_labels)\n",
    "print('\\n')\n",
    "print('accuracy=',scores[1])\n",
    "\n",
    "# Get predictions from model\n",
    "output = model.predict_classes(test_features)\n",
    "\n",
    "# Show confusion matrix\n",
    "y_actu = pd.Series(test_labels, name='Actual')\n",
    "y_pred = pd.Series(np.ndarray.flatten(output), name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(df_confusion)\n",
    "\n",
    "num_positives =  (np.count_nonzero(y_actu))\n",
    "num_negatives = y_actu.size - num_positives\n",
    "\n",
    "TN = df_confusion[0][0]\n",
    "TP = df_confusion[1][1]\n",
    "FN = df_confusion[0][1]\n",
    "FP = df_confusion[1][0]\n",
    "\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "specificity = TN/(TN + FP)\n",
    "f1 = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Num Fraud: {}\".format(num_positives))\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall/Sensitivity: \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"F1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c80c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoriawu\\Anaconda3\\envs\\MSFTHackathon2021\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0. 1.], y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Cost sensitive\n",
    "class_weights = class_weight.compute_class_weight(\"balanced\", np.unique(train_labels), train_labels)\n",
    "class_weights = {i : class_weights[i] for i in range(2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b56eccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(units=100, kernel_initializer='uniform', input_dim=train_features.shape[1], activation='tanh'))\n",
    "model2.add(Dropout(.2))\n",
    "model2.add(Dense(units=100, kernel_initializer='uniform', activation='tanh'))\n",
    "model2.add(Dropout(.2))\n",
    "model2.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "917b1a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.6699 - accuracy: 0.6369 - val_loss: 0.6701 - val_accuracy: 0.8166\n",
      "Epoch 2/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.6168 - accuracy: 0.7938 - val_loss: 0.6547 - val_accuracy: 0.8183\n",
      "Epoch 3/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.5783 - accuracy: 0.8349 - val_loss: 0.4064 - val_accuracy: 0.9044\n",
      "Epoch 4/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.5551 - accuracy: 0.8430 - val_loss: 0.5115 - val_accuracy: 0.8741\n",
      "Epoch 5/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.5385 - accuracy: 0.8445 - val_loss: 0.5918 - val_accuracy: 0.8540\n",
      "Epoch 6/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.5203 - accuracy: 0.8485 - val_loss: 0.4155 - val_accuracy: 0.8878\n",
      "Epoch 7/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.5055 - accuracy: 0.8469 - val_loss: 0.6216 - val_accuracy: 0.8238\n",
      "Epoch 8/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.4972 - accuracy: 0.8464 - val_loss: 0.3917 - val_accuracy: 0.8850\n",
      "Epoch 9/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4817 - accuracy: 0.8465 - val_loss: 0.6223 - val_accuracy: 0.8062\n",
      "Epoch 10/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4724 - accuracy: 0.8488 - val_loss: 0.6569 - val_accuracy: 0.7725\n",
      "Epoch 11/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4618 - accuracy: 0.8490 - val_loss: 0.4399 - val_accuracy: 0.8634\n",
      "Epoch 12/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4522 - accuracy: 0.8475 - val_loss: 0.4710 - val_accuracy: 0.8498\n",
      "Epoch 13/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.4418 - accuracy: 0.8486 - val_loss: 0.4387 - val_accuracy: 0.8498\n",
      "Epoch 14/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.4330 - accuracy: 0.8518 - val_loss: 0.2789 - val_accuracy: 0.8984\n",
      "Epoch 15/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.4265 - accuracy: 0.8507 - val_loss: 0.4513 - val_accuracy: 0.8559\n",
      "Epoch 16/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4186 - accuracy: 0.8529 - val_loss: 0.4828 - val_accuracy: 0.8374\n",
      "Epoch 17/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4127 - accuracy: 0.8552 - val_loss: 0.4383 - val_accuracy: 0.8546\n",
      "Epoch 18/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4076 - accuracy: 0.8559 - val_loss: 0.4048 - val_accuracy: 0.8628\n",
      "Epoch 19/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.4014 - accuracy: 0.8562 - val_loss: 0.4289 - val_accuracy: 0.8559\n",
      "Epoch 20/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.4004 - accuracy: 0.8569 - val_loss: 0.4496 - val_accuracy: 0.8409\n",
      "Epoch 21/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3943 - accuracy: 0.8581 - val_loss: 0.4975 - val_accuracy: 0.8147\n",
      "Epoch 22/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3924 - accuracy: 0.8574 - val_loss: 0.3798 - val_accuracy: 0.8703\n",
      "Epoch 23/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3894 - accuracy: 0.8573 - val_loss: 0.3887 - val_accuracy: 0.8678\n",
      "Epoch 24/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3891 - accuracy: 0.8577 - val_loss: 0.3939 - val_accuracy: 0.8581\n",
      "Epoch 25/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3839 - accuracy: 0.8587 - val_loss: 0.2822 - val_accuracy: 0.8993\n",
      "Epoch 26/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3827 - accuracy: 0.8584 - val_loss: 0.3517 - val_accuracy: 0.8738\n",
      "Epoch 27/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3789 - accuracy: 0.8586 - val_loss: 0.3847 - val_accuracy: 0.8533\n",
      "Epoch 28/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3780 - accuracy: 0.8573 - val_loss: 0.3832 - val_accuracy: 0.8558\n",
      "Epoch 29/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3724 - accuracy: 0.8596 - val_loss: 0.2587 - val_accuracy: 0.9045\n",
      "Epoch 30/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3767 - accuracy: 0.8575 - val_loss: 0.3659 - val_accuracy: 0.8459\n",
      "Epoch 31/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3731 - accuracy: 0.8555 - val_loss: 0.3089 - val_accuracy: 0.8896\n",
      "Epoch 32/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3741 - accuracy: 0.8575 - val_loss: 0.2375 - val_accuracy: 0.9096\n",
      "Epoch 33/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3672 - accuracy: 0.8576 - val_loss: 0.5062 - val_accuracy: 0.8096\n",
      "Epoch 34/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3664 - accuracy: 0.8577 - val_loss: 0.3717 - val_accuracy: 0.8402\n",
      "Epoch 35/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3693 - accuracy: 0.8560 - val_loss: 0.4627 - val_accuracy: 0.8203\n",
      "Epoch 36/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3665 - accuracy: 0.8549 - val_loss: 0.3974 - val_accuracy: 0.8297\n",
      "Epoch 37/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3625 - accuracy: 0.8552 - val_loss: 0.2971 - val_accuracy: 0.8855\n",
      "Epoch 38/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3640 - accuracy: 0.8547 - val_loss: 0.3365 - val_accuracy: 0.8547\n",
      "Epoch 39/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3615 - accuracy: 0.8556 - val_loss: 0.3408 - val_accuracy: 0.8490\n",
      "Epoch 40/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3638 - accuracy: 0.8537 - val_loss: 0.3938 - val_accuracy: 0.8262\n",
      "Epoch 41/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3601 - accuracy: 0.8546 - val_loss: 0.3332 - val_accuracy: 0.8653\n",
      "Epoch 42/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3578 - accuracy: 0.8530 - val_loss: 0.2821 - val_accuracy: 0.8893\n",
      "Epoch 43/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3575 - accuracy: 0.8520 - val_loss: 0.3113 - val_accuracy: 0.8662\n",
      "Epoch 44/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3573 - accuracy: 0.8531 - val_loss: 0.4089 - val_accuracy: 0.8234\n",
      "Epoch 45/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3561 - accuracy: 0.8527 - val_loss: 0.4673 - val_accuracy: 0.8190\n",
      "Epoch 46/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3551 - accuracy: 0.8530 - val_loss: 0.2983 - val_accuracy: 0.8707\n",
      "Epoch 47/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3555 - accuracy: 0.8513 - val_loss: 0.3184 - val_accuracy: 0.8532\n",
      "Epoch 48/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3559 - accuracy: 0.8507 - val_loss: 0.2947 - val_accuracy: 0.8715\n",
      "Epoch 49/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3569 - accuracy: 0.8512 - val_loss: 0.3775 - val_accuracy: 0.8269\n",
      "Epoch 50/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3524 - accuracy: 0.8521 - val_loss: 0.2888 - val_accuracy: 0.8849\n",
      "Epoch 51/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3509 - accuracy: 0.8524 - val_loss: 0.3652 - val_accuracy: 0.8337\n",
      "Epoch 52/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3554 - accuracy: 0.8511 - val_loss: 0.2970 - val_accuracy: 0.8651\n",
      "Epoch 53/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3522 - accuracy: 0.8508 - val_loss: 0.4603 - val_accuracy: 0.8209\n",
      "Epoch 54/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3514 - accuracy: 0.8492 - val_loss: 0.4154 - val_accuracy: 0.8143\n",
      "Epoch 55/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3484 - accuracy: 0.8509 - val_loss: 0.2666 - val_accuracy: 0.8943\n",
      "Epoch 56/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3496 - accuracy: 0.8485 - val_loss: 0.3115 - val_accuracy: 0.8436\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3470 - accuracy: 0.8498 - val_loss: 0.3223 - val_accuracy: 0.8445\n",
      "Epoch 58/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3466 - accuracy: 0.8498 - val_loss: 0.2949 - val_accuracy: 0.8576\n",
      "Epoch 59/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3472 - accuracy: 0.8498 - val_loss: 0.4069 - val_accuracy: 0.8282\n",
      "Epoch 60/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3463 - accuracy: 0.8503 - val_loss: 0.3089 - val_accuracy: 0.8487\n",
      "Epoch 61/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3464 - accuracy: 0.8478 - val_loss: 0.4426 - val_accuracy: 0.8197\n",
      "Epoch 62/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3455 - accuracy: 0.8476 - val_loss: 0.3318 - val_accuracy: 0.8348\n",
      "Epoch 63/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3436 - accuracy: 0.8491 - val_loss: 0.3290 - val_accuracy: 0.8358\n",
      "Epoch 64/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3448 - accuracy: 0.8468 - val_loss: 0.6457 - val_accuracy: 0.7546\n",
      "Epoch 65/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3446 - accuracy: 0.8495 - val_loss: 0.4612 - val_accuracy: 0.8204\n",
      "Epoch 66/200\n",
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.3468 - accuracy: 0.8469 - val_loss: 0.3462 - val_accuracy: 0.8289\n",
      "Epoch 67/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3427 - accuracy: 0.8465 - val_loss: 0.3371 - val_accuracy: 0.8332\n",
      "Epoch 68/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3434 - accuracy: 0.8468 - val_loss: 0.3496 - val_accuracy: 0.8347\n",
      "Epoch 69/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3449 - accuracy: 0.8458 - val_loss: 0.4858 - val_accuracy: 0.8196\n",
      "Epoch 70/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3443 - accuracy: 0.8443 - val_loss: 0.2913 - val_accuracy: 0.8426\n",
      "Epoch 71/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3412 - accuracy: 0.8479 - val_loss: 0.3609 - val_accuracy: 0.8300\n",
      "Epoch 72/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3415 - accuracy: 0.8453 - val_loss: 0.3387 - val_accuracy: 0.8347\n",
      "Epoch 73/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3406 - accuracy: 0.8435 - val_loss: 0.3405 - val_accuracy: 0.8308\n",
      "Epoch 74/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3416 - accuracy: 0.8455 - val_loss: 0.2668 - val_accuracy: 0.8894\n",
      "Epoch 75/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3399 - accuracy: 0.8449 - val_loss: 0.2737 - val_accuracy: 0.8688\n",
      "Epoch 76/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3401 - accuracy: 0.8454 - val_loss: 0.2326 - val_accuracy: 0.9129\n",
      "Epoch 77/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3399 - accuracy: 0.8440 - val_loss: 0.3815 - val_accuracy: 0.8274\n",
      "Epoch 78/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3411 - accuracy: 0.8434 - val_loss: 0.3688 - val_accuracy: 0.8291\n",
      "Epoch 79/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3400 - accuracy: 0.8438 - val_loss: 0.2728 - val_accuracy: 0.8677\n",
      "Epoch 80/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3422 - accuracy: 0.8442 - val_loss: 0.3685 - val_accuracy: 0.8279\n",
      "Epoch 81/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3383 - accuracy: 0.8435 - val_loss: 0.5208 - val_accuracy: 0.8209\n",
      "Epoch 82/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3369 - accuracy: 0.8429 - val_loss: 0.3099 - val_accuracy: 0.8436\n",
      "Epoch 83/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3397 - accuracy: 0.8446 - val_loss: 0.2601 - val_accuracy: 0.9036\n",
      "Epoch 84/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3381 - accuracy: 0.8449 - val_loss: 0.2839 - val_accuracy: 0.8452\n",
      "Epoch 85/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3395 - accuracy: 0.8446 - val_loss: 0.2378 - val_accuracy: 0.9071\n",
      "Epoch 86/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3380 - accuracy: 0.8429 - val_loss: 0.4026 - val_accuracy: 0.8241\n",
      "Epoch 87/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3378 - accuracy: 0.8416 - val_loss: 0.2911 - val_accuracy: 0.8464\n",
      "Epoch 88/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3367 - accuracy: 0.8437 - val_loss: 0.2416 - val_accuracy: 0.9043\n",
      "Epoch 89/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3359 - accuracy: 0.8435 - val_loss: 0.2941 - val_accuracy: 0.8426\n",
      "Epoch 90/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3363 - accuracy: 0.8421 - val_loss: 0.4002 - val_accuracy: 0.8251\n",
      "Epoch 91/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3351 - accuracy: 0.8430 - val_loss: 0.3137 - val_accuracy: 0.8366\n",
      "Epoch 92/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3364 - accuracy: 0.8432 - val_loss: 0.3648 - val_accuracy: 0.8312\n",
      "Epoch 93/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3361 - accuracy: 0.8440 - val_loss: 0.3068 - val_accuracy: 0.8367\n",
      "Epoch 94/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3361 - accuracy: 0.8425 - val_loss: 0.3218 - val_accuracy: 0.8361\n",
      "Epoch 95/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3349 - accuracy: 0.8424 - val_loss: 0.2479 - val_accuracy: 0.9084\n",
      "Epoch 96/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3346 - accuracy: 0.8416 - val_loss: 0.3450 - val_accuracy: 0.8312\n",
      "Epoch 97/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3351 - accuracy: 0.8426 - val_loss: 0.3230 - val_accuracy: 0.8386\n",
      "Epoch 98/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3354 - accuracy: 0.8428 - val_loss: 0.2991 - val_accuracy: 0.8421\n",
      "Epoch 99/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3313 - accuracy: 0.8427 - val_loss: 0.3873 - val_accuracy: 0.8309\n",
      "Epoch 100/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3347 - accuracy: 0.8431 - val_loss: 0.3616 - val_accuracy: 0.8294\n",
      "Epoch 101/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3339 - accuracy: 0.8439 - val_loss: 0.3894 - val_accuracy: 0.8281\n",
      "Epoch 102/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3341 - accuracy: 0.8423 - val_loss: 0.4014 - val_accuracy: 0.8290\n",
      "Epoch 103/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3324 - accuracy: 0.8412 - val_loss: 0.3169 - val_accuracy: 0.8347\n",
      "Epoch 104/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3337 - accuracy: 0.8422 - val_loss: 0.3286 - val_accuracy: 0.8327\n",
      "Epoch 105/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3322 - accuracy: 0.8410 - val_loss: 0.2940 - val_accuracy: 0.8448\n",
      "Epoch 106/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3334 - accuracy: 0.8421 - val_loss: 0.2891 - val_accuracy: 0.8453\n",
      "Epoch 107/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3335 - accuracy: 0.8422 - val_loss: 0.3112 - val_accuracy: 0.8381\n",
      "Epoch 108/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3315 - accuracy: 0.8424 - val_loss: 0.2664 - val_accuracy: 0.8919\n",
      "Epoch 109/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3344 - accuracy: 0.8424 - val_loss: 0.4428 - val_accuracy: 0.8250\n",
      "Epoch 110/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3318 - accuracy: 0.8411 - val_loss: 0.3799 - val_accuracy: 0.8308\n",
      "Epoch 111/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3321 - accuracy: 0.8423 - val_loss: 0.4564 - val_accuracy: 0.8266\n",
      "Epoch 112/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3317 - accuracy: 0.8414 - val_loss: 0.3859 - val_accuracy: 0.8308\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2371/2371 [==============================] - 5s 2ms/step - loss: 0.3327 - accuracy: 0.8412 - val_loss: 0.2857 - val_accuracy: 0.8506\n",
      "Epoch 114/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3312 - accuracy: 0.8428 - val_loss: 0.2942 - val_accuracy: 0.8425\n",
      "Epoch 115/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3303 - accuracy: 0.8422 - val_loss: 0.2981 - val_accuracy: 0.8478\n",
      "Epoch 116/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3322 - accuracy: 0.8418 - val_loss: 0.2882 - val_accuracy: 0.8475\n",
      "Epoch 117/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3310 - accuracy: 0.8416 - val_loss: 0.3226 - val_accuracy: 0.8322\n",
      "Epoch 118/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3300 - accuracy: 0.8418 - val_loss: 0.2628 - val_accuracy: 0.8831\n",
      "Epoch 119/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3299 - accuracy: 0.8405 - val_loss: 0.3042 - val_accuracy: 0.8375\n",
      "Epoch 120/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3291 - accuracy: 0.8424 - val_loss: 0.3498 - val_accuracy: 0.8321\n",
      "Epoch 121/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3297 - accuracy: 0.8419 - val_loss: 0.4610 - val_accuracy: 0.8221\n",
      "Epoch 122/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3308 - accuracy: 0.8414 - val_loss: 0.3780 - val_accuracy: 0.8292\n",
      "Epoch 123/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3308 - accuracy: 0.8410 - val_loss: 0.3834 - val_accuracy: 0.8281\n",
      "Epoch 124/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3300 - accuracy: 0.8426 - val_loss: 0.2914 - val_accuracy: 0.8499\n",
      "Epoch 125/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3299 - accuracy: 0.8404 - val_loss: 0.3485 - val_accuracy: 0.8367\n",
      "Epoch 126/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3287 - accuracy: 0.8414 - val_loss: 0.3270 - val_accuracy: 0.8346\n",
      "Epoch 127/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3293 - accuracy: 0.8408 - val_loss: 0.3261 - val_accuracy: 0.8334\n",
      "Epoch 128/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3297 - accuracy: 0.8405 - val_loss: 0.4124 - val_accuracy: 0.8273\n",
      "Epoch 129/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3288 - accuracy: 0.8415 - val_loss: 0.3305 - val_accuracy: 0.8341\n",
      "Epoch 130/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3275 - accuracy: 0.8426 - val_loss: 0.2807 - val_accuracy: 0.8461\n",
      "Epoch 131/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3290 - accuracy: 0.8406 - val_loss: 0.2725 - val_accuracy: 0.8540\n",
      "Epoch 132/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3303 - accuracy: 0.8407 - val_loss: 0.3215 - val_accuracy: 0.8315\n",
      "Epoch 133/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3293 - accuracy: 0.8416 - val_loss: 0.3074 - val_accuracy: 0.8422\n",
      "Epoch 134/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3283 - accuracy: 0.8412 - val_loss: 0.3305 - val_accuracy: 0.8336\n",
      "Epoch 135/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3293 - accuracy: 0.8407 - val_loss: 0.3139 - val_accuracy: 0.8365\n",
      "Epoch 136/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3281 - accuracy: 0.8413 - val_loss: 0.2759 - val_accuracy: 0.8621\n",
      "Epoch 137/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3274 - accuracy: 0.8421 - val_loss: 0.3678 - val_accuracy: 0.8249\n",
      "Epoch 138/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3269 - accuracy: 0.8423 - val_loss: 0.3159 - val_accuracy: 0.8374\n",
      "Epoch 139/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3272 - accuracy: 0.8422 - val_loss: 0.3385 - val_accuracy: 0.8337\n",
      "Epoch 140/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3279 - accuracy: 0.8405 - val_loss: 0.3315 - val_accuracy: 0.8305\n",
      "Epoch 141/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3279 - accuracy: 0.8433 - val_loss: 0.3237 - val_accuracy: 0.8332\n",
      "Epoch 142/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3290 - accuracy: 0.8415 - val_loss: 0.3148 - val_accuracy: 0.8332\n",
      "Epoch 143/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3266 - accuracy: 0.8419 - val_loss: 0.3559 - val_accuracy: 0.8316\n",
      "Epoch 144/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3284 - accuracy: 0.8403 - val_loss: 0.3972 - val_accuracy: 0.8202\n",
      "Epoch 145/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3275 - accuracy: 0.8427 - val_loss: 0.3847 - val_accuracy: 0.8303\n",
      "Epoch 146/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3288 - accuracy: 0.8406 - val_loss: 0.3331 - val_accuracy: 0.8328\n",
      "Epoch 147/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3278 - accuracy: 0.8419 - val_loss: 0.3094 - val_accuracy: 0.8448\n",
      "Epoch 148/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3273 - accuracy: 0.8406 - val_loss: 0.3219 - val_accuracy: 0.8318\n",
      "Epoch 149/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3270 - accuracy: 0.8402 - val_loss: 0.3262 - val_accuracy: 0.8381\n",
      "Epoch 150/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3273 - accuracy: 0.8413 - val_loss: 0.3318 - val_accuracy: 0.8352\n",
      "Epoch 151/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3289 - accuracy: 0.8398 - val_loss: 0.2965 - val_accuracy: 0.8404\n",
      "Epoch 152/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3278 - accuracy: 0.8407 - val_loss: 0.2996 - val_accuracy: 0.8354\n",
      "Epoch 153/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3293 - accuracy: 0.8411 - val_loss: 0.3833 - val_accuracy: 0.8313\n",
      "Epoch 154/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3266 - accuracy: 0.8415 - val_loss: 0.2976 - val_accuracy: 0.8430\n",
      "Epoch 155/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3258 - accuracy: 0.8411 - val_loss: 0.3729 - val_accuracy: 0.8307\n",
      "Epoch 156/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3266 - accuracy: 0.8417 - val_loss: 0.3170 - val_accuracy: 0.8395\n",
      "Epoch 157/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3264 - accuracy: 0.8408 - val_loss: 0.3191 - val_accuracy: 0.8327\n",
      "Epoch 158/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3275 - accuracy: 0.8416 - val_loss: 0.3386 - val_accuracy: 0.8340\n",
      "Epoch 159/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3255 - accuracy: 0.8416 - val_loss: 0.3087 - val_accuracy: 0.8466\n",
      "Epoch 160/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3269 - accuracy: 0.8401 - val_loss: 0.3316 - val_accuracy: 0.8327\n",
      "Epoch 161/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3247 - accuracy: 0.8413 - val_loss: 0.3144 - val_accuracy: 0.8434\n",
      "Epoch 162/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3247 - accuracy: 0.8406 - val_loss: 0.3147 - val_accuracy: 0.8326\n",
      "Epoch 163/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3255 - accuracy: 0.8418 - val_loss: 0.4010 - val_accuracy: 0.8293\n",
      "Epoch 164/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3264 - accuracy: 0.8419 - val_loss: 0.2934 - val_accuracy: 0.8468\n",
      "Epoch 165/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3252 - accuracy: 0.8422 - val_loss: 0.2764 - val_accuracy: 0.8714\n",
      "Epoch 166/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3252 - accuracy: 0.8414 - val_loss: 0.3267 - val_accuracy: 0.8379\n",
      "Epoch 167/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3264 - accuracy: 0.8422 - val_loss: 0.3143 - val_accuracy: 0.8413\n",
      "Epoch 168/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3260 - accuracy: 0.8416 - val_loss: 0.3380 - val_accuracy: 0.8327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3266 - accuracy: 0.8407 - val_loss: 0.3344 - val_accuracy: 0.8325\n",
      "Epoch 170/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3252 - accuracy: 0.8413 - val_loss: 0.3370 - val_accuracy: 0.8329\n",
      "Epoch 171/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3253 - accuracy: 0.8418 - val_loss: 0.2798 - val_accuracy: 0.8674\n",
      "Epoch 172/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3258 - accuracy: 0.8419 - val_loss: 0.3666 - val_accuracy: 0.8322\n",
      "Epoch 173/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3256 - accuracy: 0.8424 - val_loss: 0.3462 - val_accuracy: 0.8349\n",
      "Epoch 174/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3262 - accuracy: 0.8417 - val_loss: 0.3148 - val_accuracy: 0.8348\n",
      "Epoch 175/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3258 - accuracy: 0.8405 - val_loss: 0.2897 - val_accuracy: 0.8473\n",
      "Epoch 176/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3259 - accuracy: 0.8398 - val_loss: 0.3364 - val_accuracy: 0.8332\n",
      "Epoch 177/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3236 - accuracy: 0.8409 - val_loss: 0.3291 - val_accuracy: 0.8427\n",
      "Epoch 178/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3249 - accuracy: 0.8423 - val_loss: 0.3107 - val_accuracy: 0.8442\n",
      "Epoch 179/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3241 - accuracy: 0.8419 - val_loss: 0.3011 - val_accuracy: 0.8551\n",
      "Epoch 180/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3237 - accuracy: 0.8426 - val_loss: 0.3265 - val_accuracy: 0.8368\n",
      "Epoch 181/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3255 - accuracy: 0.8411 - val_loss: 0.3288 - val_accuracy: 0.8385\n",
      "Epoch 182/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3245 - accuracy: 0.8430 - val_loss: 0.3232 - val_accuracy: 0.8326\n",
      "Epoch 183/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3248 - accuracy: 0.8417 - val_loss: 0.3107 - val_accuracy: 0.8369\n",
      "Epoch 184/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3254 - accuracy: 0.8412 - val_loss: 0.3449 - val_accuracy: 0.8317\n",
      "Epoch 185/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3234 - accuracy: 0.8426 - val_loss: 0.3084 - val_accuracy: 0.8462\n",
      "Epoch 186/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3238 - accuracy: 0.8430 - val_loss: 0.3666 - val_accuracy: 0.8305\n",
      "Epoch 187/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3242 - accuracy: 0.8417 - val_loss: 0.3426 - val_accuracy: 0.8310\n",
      "Epoch 188/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3242 - accuracy: 0.8429 - val_loss: 0.3152 - val_accuracy: 0.8345\n",
      "Epoch 189/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3242 - accuracy: 0.8417 - val_loss: 0.2692 - val_accuracy: 0.8834\n",
      "Epoch 190/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3235 - accuracy: 0.8434 - val_loss: 0.2399 - val_accuracy: 0.9117\n",
      "Epoch 191/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3237 - accuracy: 0.8428 - val_loss: 0.3135 - val_accuracy: 0.8436\n",
      "Epoch 192/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3231 - accuracy: 0.8429 - val_loss: 0.2972 - val_accuracy: 0.8481\n",
      "Epoch 193/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3233 - accuracy: 0.8428 - val_loss: 0.3558 - val_accuracy: 0.8320\n",
      "Epoch 194/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3252 - accuracy: 0.8410 - val_loss: 0.3234 - val_accuracy: 0.8417\n",
      "Epoch 195/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3229 - accuracy: 0.8432 - val_loss: 0.2949 - val_accuracy: 0.8536\n",
      "Epoch 196/200\n",
      "2371/2371 [==============================] - 6s 3ms/step - loss: 0.3243 - accuracy: 0.8415 - val_loss: 0.2813 - val_accuracy: 0.8524\n",
      "Epoch 197/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3240 - accuracy: 0.8432 - val_loss: 0.2618 - val_accuracy: 0.9116\n",
      "Epoch 198/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3238 - accuracy: 0.8412 - val_loss: 0.2822 - val_accuracy: 0.8620\n",
      "Epoch 199/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3230 - accuracy: 0.8423 - val_loss: 0.3103 - val_accuracy: 0.8413\n",
      "Epoch 200/200\n",
      "2371/2371 [==============================] - 6s 2ms/step - loss: 0.3249 - accuracy: 0.8413 - val_loss: 0.3219 - val_accuracy: 0.8343\n"
     ]
    }
   ],
   "source": [
    "train_history2 = model2.fit(train_features, train_labels, batch_size=50, epochs=200, verbose=1, validation_split=0.2, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "851ddcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158/1158 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.83 - 1s 1ms/step - loss: 0.3210 - accuracy: 0.8354\n",
      "\n",
      "\n",
      "accuracy= 0.8354119658470154\n",
      "Predicted      0     1\n",
      "Actual                \n",
      "0.0        28148  5921\n",
      "1.0          176  2799\n",
      "Num Fraud: 2975\n",
      "Precision:  0.3209862385321101\n",
      "Recall/Sensitivity:  0.9408403361344538\n",
      "Specificity:  0.8262056414922657\n",
      "F1 Score:  0.4786660966224882\n"
     ]
    }
   ],
   "source": [
    "# Get model accuracy \n",
    "scores = model2.evaluate(test_features, test_labels)\n",
    "print('\\n')\n",
    "print('accuracy=',scores[1])\n",
    "\n",
    "# Get predictions from model\n",
    "output = model2.predict_classes(test_features)\n",
    "\n",
    "# Show confusion matrix\n",
    "y_actu = pd.Series(test_labels, name='Actual')\n",
    "y_pred = pd.Series(np.ndarray.flatten(output), name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(df_confusion)\n",
    "\n",
    "num_positives =  (np.count_nonzero(y_actu))\n",
    "num_negatives = y_actu.size - num_positives\n",
    "\n",
    "TN = df_confusion[0][0]\n",
    "TP = df_confusion[1][1]\n",
    "FN = df_confusion[0][1]\n",
    "FP = df_confusion[1][0]\n",
    "\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "specificity = TN/(TN + FP)\n",
    "f1 = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Num Fraud: {}\".format(num_positives))\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall/Sensitivity: \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"F1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e223431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
